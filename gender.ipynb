{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: utils in c:\\users\\disha\\anaconda3\\lib\\site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-29832fcd1ba3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-1e0e0f2e5ad7>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-16-1e0e0f2e5ad7>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    git clone https://github.com/x4nth055/gender-recognition-by-voice\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for numpy: [Errno 2] No such file or directory: 'c:\\\\users\\\\disha\\\\anaconda3\\\\lib\\\\site-packages\\\\numpy-1.19.2.dist-info\\\\METADATA'\n",
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\disha\\\\anaconda3\\\\Lib\\\\site-packages\\\\grpc\\\\framework\\\\common\\\\style.py'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.8.0-cp38-cp38-win_amd64.whl (438.0 MB)\n",
      "Collecting keras<2.9,>=2.8.0rc0\n",
      "  Using cached keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Using cached libclang-13.0.0-py2.py3-none-win_amd64.whl (13.9 MB)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Using cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting gast>=0.2.1\n",
      "  Using cached gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Processing c:\\users\\disha\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\\termcolor-1.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.44.0-cp38-cp38-win_amd64.whl (3.4 MB)\n",
      "Collecting flatbuffers>=1.12\n",
      "  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.24.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Using cached tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "Collecting numpy>=1.20\n",
      "  Using cached numpy-1.22.3-cp38-cp38-win_amd64.whl (14.7 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Using cached protobuf-3.20.0-cp38-cp38-win_amd64.whl (904 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\disha\\anaconda3\\lib\\site-packages (from tensorflow) (50.3.1.post20201107)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.35.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.6.5-py2.py3-none-any.whl (156 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
      "Collecting importlib-metadata>=4.4; python_version < \"3.10\"\n",
      "  Using cached importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Using cached rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.4.0)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Installing collected packages: keras, libclang, absl-py, protobuf, tensorboard-data-server, tensorboard-plugin-wit, importlib-metadata, markdown, grpcio, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, numpy, tensorboard, gast, opt-einsum, google-pasta, termcolor, flatbuffers, tensorflow-io-gcs-filesystem, keras-preprocessing, tf-estimator-nightly, astunparse, tensorflow\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 2.0.0\n",
      "    Can't uninstall 'importlib-metadata'. No files were found to uninstall.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for numpy: [Errno 2] No such file or directory: 'c:\\\\users\\\\disha\\\\anaconda3\\\\lib\\\\site-packages\\\\numpy-1.19.2.dist-info\\\\METADATA'\n",
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\disha\\\\anaconda3\\\\Lib\\\\site-packages\\\\keras\\\\saving\\\\saved_model\\\\base_serialization.py'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.8.0-cp38-cp38-win_amd64.whl (438.0 MB)\n",
      "Processing c:\\users\\disha\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\\termcolor-1.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in c:\\users\\disha\\anaconda3\\lib\\site-packages (from tensorflow) (50.3.1.post20201107)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting libclang>=9.0.1\n",
      "  Using cached libclang-13.0.0-py2.py3-none-win_amd64.whl (13.9 MB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Using cached protobuf-3.20.0-cp38-cp38-win_amd64.whl (904 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.24.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Using cached tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Using cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting keras<2.9,>=2.8.0rc0\n",
      "  Using cached keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting gast>=0.2.1\n",
      "  Using cached gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.44.0-cp38-cp38-win_amd64.whl (3.4 MB)\n",
      "Collecting flatbuffers>=1.12\n",
      "  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting numpy>=1.20\n",
      "  Using cached numpy-1.22.3-cp38-cp38-win_amd64.whl (14.7 MB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.6.5-py2.py3-none-any.whl (156 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.35.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.24.0)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Using cached rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4; python_version < \"3.10\"\n",
      "  Using cached importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.4.0)\n",
      "Installing collected packages: termcolor, libclang, protobuf, tensorflow-io-gcs-filesystem, tf-estimator-nightly, numpy, keras-preprocessing, google-pasta, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, tensorboard-plugin-wit, oauthlib, requests-oauthlib, google-auth-oauthlib, grpcio, importlib-metadata, markdown, absl-py, tensorboard-data-server, tensorboard, keras, opt-einsum, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.2\n",
      "    Can't uninstall 'numpy'. No files were found to uninstall.\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 2.0.0\n",
      "    Can't uninstall 'importlib-metadata'. No files were found to uninstall.\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.8.0-cp38-cp38-win_amd64.whl (438.0 MB)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Collecting numpy>=1.20\n",
      "  Downloading numpy-1.22.3-cp38-cp38-win_amd64.whl (14.7 MB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.20.0-cp38-cp38-win_amd64.whl (904 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Collecting gast>=0.2.1\n",
      "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting flatbuffers>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-13.0.0-py2.py3-none-win_amd64.whl (13.9 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\disha\\anaconda3\\lib\\site-packages (from tensorflow) (50.3.1.post20201107)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.44.0-cp38-cp38-win_amd64.whl (3.4 MB)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting keras<2.9,>=2.8.0rc0\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.24.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.24.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.35.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.5-py2.py3-none-any.whl (156 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.25.11)\n",
      "Collecting importlib-metadata>=4.4; python_version < \"3.10\"\n",
      "  Downloading importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.4.0)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4835 sha256=b88ac7e498fa76c758ea85d75b6cf6da94c69104a75dff2f152f989896afdce7\n",
      "  Stored in directory: c:\\users\\disha\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built termcolor\n",
      "Installing collected packages: absl-py, numpy, protobuf, gast, google-pasta, flatbuffers, importlib-metadata, markdown, tensorboard-plugin-wit, oauthlib, requests-oauthlib, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, google-auth-oauthlib, grpcio, tensorboard-data-server, tensorboard, libclang, opt-einsum, tf-estimator-nightly, termcolor, astunparse, keras, tensorflow-io-gcs-filesystem, keras-preprocessing, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.2\n",
      "    Uninstalling numpy-1.19.2:\n",
      "      Successfully uninstalled numpy-1.19.2\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 2.0.0\n",
      "    Uninstalling importlib-metadata-2.0.0:\n",
      "      Successfully uninstalled importlib-metadata-2.0.0\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.0.0 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.5 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.44.0 importlib-metadata-4.11.3 keras-2.8.0 keras-preprocessing-1.1.2 libclang-13.0.0 markdown-3.3.6 numpy-1.22.3 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.20.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.24.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "import requests\n",
    "url = \"https://raw.githubusercontent.com/x4nth055/gender-recognition-by-voice/master/balanced-all.csv\"\n",
    "res = requests.get(url, allow_redirects=True)\n",
    "with open('sales_team.csv','wb') as file:\n",
    "    file.write(res.content)\n",
    "voice_data = pd.read_csv('sales_team.csv')\n",
    "def load_data(vector_length=128):\n",
    "    \"\"\"A function to load gender recognition dataset from `data` folder\n",
    "    After the second run, this will load from results/features.npy and results/labels.npy files\n",
    "    as it is much faster!\"\"\"\n",
    "    # make sure results folder exists\n",
    "    if not os.path.isdir(\"results\"):\n",
    "        os.mkdir(\"results\")\n",
    "    # if features & labels already loaded individually and bundled, load them from there instead\n",
    "    if os.path.isfile(\"results/features.npy\") and os.path.isfile(\"results/labels.npy\"):\n",
    "        X = np.load(\"results/features.npy\")\n",
    "        y = np.load(\"results/labels.npy\")\n",
    "        return X, y\n",
    "    # read dataframe\n",
    "    df = pd.read_csv(\"balanced-all.csv\")\n",
    "    # get total samples\n",
    "    n_samples = len(df)\n",
    "    # get total male samples\n",
    "    n_male_samples = len(df[df['gender'] == 'male'])\n",
    "    # get total female samples\n",
    "    n_female_samples = len(df[df['gender'] == 'female'])\n",
    "    print(\"Total samples:\", n_samples)\n",
    "    print(\"Total male samples:\", n_male_samples)\n",
    "    print(\"Total female samples:\", n_female_samples)\n",
    "    # initialize an empty array for all audio features\n",
    "    X = np.zeros((n_samples, vector_length))\n",
    "    # initialize an empty array for all audio labels (1 for male and 0 for female)\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    for i, (filename, gender) in tqdm.tqdm(enumerate(zip(df['filename'], df['gender'])), \"Loading data\", total=n_samples):\n",
    "        features = np.load(filename)\n",
    "        X[i] = features\n",
    "        y[i] = label2int[gender]\n",
    "    # save the audio features and labels into files\n",
    "    # so we won't load each one of them next run\n",
    "    np.save(\"results/features\", X)\n",
    "    np.save(\"results/labels\", y)\n",
    "    return X, y\n",
    "\n",
    "def split_data(X, y, test_size=0.1, valid_size=0.1):\n",
    "    # split training set and testing set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=7)\n",
    "    # split training set and validation set\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=valid_size, random_state=7)\n",
    "    # return a dictionary of values\n",
    "    return {\n",
    "        \"X_train\": X_train,\n",
    "        \"X_valid\": X_valid,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_valid\": y_valid,\n",
    "        \"y_test\": y_test\n",
    "    }\n",
    "\n",
    "\n",
    "def create_model(vector_length=128):\n",
    "    \"\"\"5 hidden dense layers from 256 units to 64, not the best model, but not bad.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_shape=(vector_length,)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    # one output neuron with sigmoid activation function, 0 means female, 1 means male\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    # using binary crossentropy as it's male/female classification (binary)\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"], optimizer=\"adam\")\n",
    "    # print summary of the model\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_data' from 'utils' (C:\\Users\\disha\\anaconda3\\lib\\site-packages\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-15d8b9b2aadb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# load the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'load_data' from 'utils' (C:\\Users\\disha\\anaconda3\\lib\\site-packages\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from requests) (2020.6.20)\n",
      "Collecting charset-normalizer~=2.0.0; python_version >= \"3\"\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.21.1 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied, skipping upgrade: idna<4,>=2.5; python_version >= \"3\" in c:\\users\\disha\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Installing collected packages: charset-normalizer, requests\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.24.0\n",
      "    Uninstalling requests-2.24.0:\n",
      "      Successfully uninstalled requests-2.24.0\n",
      "Successfully installed charset-normalizer-2.0.12 requests-2.27.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_data' from 'utils' (C:\\Users\\disha\\anaconda3\\lib\\site-packages\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5479c3790a16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# load the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'load_data' from 'utils' (C:\\Users\\disha\\anaconda3\\lib\\site-packages\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from utils import load_data, split_data, create_model\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_data()\n",
    "# split the data into training, validation and testing sets\n",
    "data = split_data(X, y, test_size=0.1, valid_size=0.1)\n",
    "# construct the model\n",
    "model = create_model()\n",
    "# use tensorboard to view metrics\n",
    "tensorboard = TensorBoard(log_dir=\"logs\")\n",
    "# define early stopping to stop training after 5 epochs of not improving\n",
    "early_stopping = EarlyStopping(mode=\"min\", patience=5, restore_best_weights=True)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "# train the model using the training set and validating using validation set\n",
    "model.fit(data[\"X_train\"], data[\"y_train\"], epochs=epochs, batch_size=batch_size, validation_data=(data[\"X_valid\"], data[\"y_valid\"]),\n",
    "          callbacks=[tensorboard, early_stopping])\n",
    "\n",
    "# save the model to a file\n",
    "model.save(\"results/model.h5\")\n",
    "\n",
    "# evaluating the model using the testing set\n",
    "print(f\"Evaluating the model using {len(data['X_test'])} samples...\")\n",
    "loss, accuracy = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: utils in c:\\users\\disha\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_data' from 'utils' (C:\\Users\\disha\\anaconda3\\lib\\site-packages\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-897968fbefbb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'load_data' from 'utils' (C:\\Users\\disha\\anaconda3\\lib\\site-packages\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from utils import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000028C563AB100>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pyaudio/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000028C563A8E50>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pyaudio/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000028C563A8D30>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pyaudio/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000028C563A8BE0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pyaudio/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000028C563A88E0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pyaudio/\n",
      "ERROR: Could not find a version that satisfies the requirement pyaudio (from versions: none)\n",
      "ERROR: No matching distribution found for pyaudio\n"
     ]
    }
   ],
   "source": [
    "pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyaudio in c:\\users\\disha\\anaconda3\\lib\\site-packages (0.2.11)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_data' from 'utils' (C:\\Users\\disha\\anaconda3\\lib\\site-packages\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-897968fbefbb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'load_data' from 'utils' (C:\\Users\\disha\\anaconda3\\lib\\site-packages\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from utils import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'librosa'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1edc72e33cdd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwave\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbyteorder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'librosa'"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import os\n",
    "import wave\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sys import byteorder\n",
    "from array import array\n",
    "from struct import pack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Using cached librosa-0.9.1-py3-none-any.whl (213 kB)\n",
      "Requirement already satisfied: numba>=0.45.1 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from librosa) (0.51.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from librosa) (1.22.3)\n",
      "Processing c:\\users\\disha\\appdata\\local\\pip\\cache\\wheels\\6f\\d1\\5d\\f13da53b1dcbc2624ff548456c9ffb526c914f53c12c318bb4\\resampy-0.2.2-py3-none-any.whl\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from librosa) (20.4)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from librosa) (0.23.2)\n",
      "Requirement already satisfied: decorator>=4.0.10 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from librosa) (4.4.2)\n",
      "Processing c:\\users\\disha\\appdata\\local\\pip\\cache\\wheels\\49\\5a\\e4\\df590783499a992a88de6c0898991d1167453a3196d0d1eeb7\\audioread-2.1.9-py3-none-any.whl\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from librosa) (1.5.2)\n",
      "Collecting pooch>=1.0\n",
      "  Using cached pooch-1.6.0-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from librosa) (0.17.0)\n",
      "Collecting soundfile>=0.10.2\n",
      "  Using cached SoundFile-0.10.3.post1-py2.py3.cp26.cp27.cp32.cp33.cp34.cp35.cp36.pp27.pp32.pp33-none-win_amd64.whl (689 kB)\n",
      "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from numba>=0.45.1->librosa) (0.34.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\disha\\anaconda3\\lib\\site-packages (from numba>=0.45.1->librosa) (50.3.1.post20201107)\n",
      "Requirement already satisfied: six>=1.3 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from packaging>=20.0->librosa) (2.4.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19.1->librosa) (2.1.0)\n",
      "Collecting appdirs>=1.3.0\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from pooch>=1.0->librosa) (2.27.1)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from soundfile>=0.10.2->librosa) (1.14.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.25.11)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in c:\\users\\disha\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in c:\\users\\disha\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.12)\n",
      "Requirement already satisfied: pycparser in c:\\users\\disha\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.20)\n",
      "Installing collected packages: resampy, audioread, appdirs, pooch, soundfile, librosa\n",
      "Successfully installed appdirs-1.4.4 audioread-2.1.9 librosa-0.9.1 pooch-1.6.0 resampy-0.2.2 soundfile-0.10.3.post1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import os\n",
    "import wave\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sys import byteorder\n",
    "from array import array\n",
    "from struct import pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement speech_recognition (from versions: none)\n",
      "ERROR: No matching distribution found for speech_recognition\n"
     ]
    }
   ],
   "source": [
    "pip install speech_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                filename  gender\n",
      "0  data/cv-other-train/sample-069205.npy  female\n",
      "1  data/cv-valid-train/sample-063134.npy  female\n",
      "2  data/cv-other-train/sample-080873.npy  female\n",
      "3  data/cv-other-train/sample-105595.npy  female\n",
      "4  data/cv-valid-train/sample-144613.npy  female\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "    \n",
    "# Downloading the csv file from your GitHub account\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/x4nth055/gender-recognition-by-voice/master/balanced-all.csv\" # Make sure the url is the raw version of the file on GitHub\n",
    "download = requests.get(url).content\n",
    "\n",
    "# Reading the downloaded content and turning it into a pandas dataframe\n",
    "\n",
    "df = pd.read_csv(io.StringIO(download.decode('utf-8')))\n",
    "\n",
    "# Printing out the first 5 rows of the dataframe\n",
    "\n",
    "print (df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Preprocessing balanced-all.csv\n",
      "Previously: 66938 rows\n",
      "Now: 66938 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features of balanced-all: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def extract_feature(file_name, **kwargs):\n",
    "    \"\"\"\n",
    "    Extract feature from audio file `file_name`\n",
    "        Features supported:\n",
    "            - MFCC (mfcc)\n",
    "            - Chroma (chroma)\n",
    "            - MEL Spectrogram Frequency (mel)\n",
    "            - Contrast (contrast)\n",
    "            - Tonnetz (tonnetz)\n",
    "        e.g:\n",
    "        `features = extract_feature(path, mel=True, mfcc=True)`\n",
    "    \"\"\"\n",
    "    mfcc = kwargs.get(\"mfcc\")\n",
    "    chroma = kwargs.get(\"chroma\")\n",
    "    mel = kwargs.get(\"mel\")\n",
    "    contrast = kwargs.get(\"contrast\")\n",
    "    tonnetz = kwargs.get(\"tonnetz\")\n",
    "    X, sample_rate = librosa.core.load(file_name)\n",
    "    if chroma or contrast:\n",
    "        stft = np.abs(librosa.stft(X))\n",
    "    result = np.array([])\n",
    "    if mfcc:\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        result = np.hstack((result, mfccs))\n",
    "    if chroma:\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, chroma))\n",
    "    if mel:\n",
    "        mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, mel))\n",
    "    if contrast:\n",
    "        contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, contrast))\n",
    "    if tonnetz:\n",
    "        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, tonnetz))\n",
    "    return result\n",
    "\n",
    "dirname = \"data\"\n",
    "\n",
    "if not os.path.isdir(dirname):\n",
    "    os.mkdir(dirname)\n",
    "\n",
    "\n",
    "csv_files = glob.glob(\"*.csv\")\n",
    "\n",
    "for j, csv_file in enumerate(csv_files):\n",
    "    print(\"[+] Preprocessing\", csv_file)\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # only take filename and gender columns\n",
    "    new_df = df[[\"filename\", \"gender\"]]\n",
    "    print(\"Previously:\", len(new_df), \"rows\")\n",
    "    # take only male & female genders (i.e droping NaNs & 'other' gender)\n",
    "    new_df = new_df[np.logical_or(new_df['gender'] == 'female', new_df['gender'] == 'male')]\n",
    "    print(\"Now:\", len(new_df), \"rows\")\n",
    "    new_csv_file = os.path.join(dirname, csv_file)\n",
    "    # save new preprocessed CSV \n",
    "    new_df.to_csv(new_csv_file, index=False)\n",
    "    # get the folder name\n",
    "    folder_name, _ = csv_file.split(\".\")\n",
    "    audio_files = glob.glob(f\"{folder_name}/{folder_name}/*\")\n",
    "    all_audio_filenames = set(new_df[\"filename\"])\n",
    "    for i, audio_file in tqdm(list(enumerate(audio_files)), f\"Extracting features of {folder_name}\"):\n",
    "        splited = os.path.split(audio_file)\n",
    "        # audio_filename = os.path.join(os.path.split(splited[0])[-1], splited[-1])\n",
    "        audio_filename = f\"{os.path.split(splited[0])[-1]}/{splited[-1]}\"\n",
    "        # print(\"audio_filename:\", audio_filename)\n",
    "        if audio_filename in all_audio_filenames:\n",
    "            # print(\"Copyying\", audio_filename, \"...\")\n",
    "            src_path = f\"{folder_name}/{audio_filename}\"\n",
    "            target_path = f\"{dirname}/{audio_filename}\"\n",
    "            #create that folder if it doesn't exist\n",
    "            if not os.path.isdir(os.path.dirname(target_path)):\n",
    "                os.mkdir(os.path.dirname(target_path))\n",
    "            features = extract_feature(src_path, mel=True)\n",
    "            target_filename = target_path.split(\".\")[0]\n",
    "            np.save(target_filename, features)\n",
    "            # shutil.copyfile(src_path, target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "    \n",
    "# Downloading the csv file from your GitHub account\n",
    "\n",
    "\n",
    "\n",
    "label2int = {\n",
    "    \"male\": 1,\n",
    "    \"female\": 0\n",
    "}\n",
    "vector_length=128\n",
    "print(type(vector_length))\n",
    "\n",
    "def load_data(vector_length=128):\n",
    "    \"\"\"A function to load gender recognition dataset from `data` folder\n",
    "    After the second run, this will load from results/features.npy and results/labels.npy files\n",
    "    as it is much faster!\"\"\"\n",
    "    # make sure results folder exists\n",
    "    if not os.path.isdir(\"results\"):\n",
    "        os.mkdir(\"results\")\n",
    "    # if features & labels already loaded individually and bundled, load them from there instead\n",
    "    if os.path.isfile(\"results/features.npy\") and os.path.isfile(\"results/labels.npy\"):\n",
    "        X = np.load(\"results/features.npy\")\n",
    "        y = np.load(\"results/labels.npy\")\n",
    "        return X, y\n",
    "    # read dataframe\n",
    "    df = pd.read_csv(\"balanced-all.csv\")\n",
    "    # get total samples\n",
    "    n_samples = len(df)\n",
    "    # get total male samples\n",
    "    n_male_samples = len(df[df['gender'] == 'male'])\n",
    "    # get total female samples\n",
    "    n_female_samples = len(df[df['gender'] == 'female'])\n",
    "    print(\"Total samples:\", n_samples)\n",
    "    print(\"Total male samples:\", n_male_samples)\n",
    "    print(\"Total female samples:\", n_female_samples)\n",
    "    # initialize an empty array for all audio features\n",
    "    X = np.zeros(n_samples, vector_length)\n",
    "    # initialize an empty array for all audio labels (1 for male and 0 for female)\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    for i, (filename, gender) in tqdm.tqdm(enumerate(zip(df['filename'], df['gender'])), \"Loading data\", total=n_samples):\n",
    "        features = np.load(filename)\n",
    "        X[i] = features\n",
    "        y[i] = label2int[gender]\n",
    "    # save the audio features and labels into files\n",
    "    # so we won't load each one of them next run\n",
    "    np.save(\"results/features\", X)\n",
    "    np.save(\"results/labels\", y)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def split_data(X, y, test_size=0.1, valid_size=0.1):\n",
    "    # split training set and testing set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=7)\n",
    "    # split training set and validation set\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=valid_size, random_state=7)\n",
    "    # return a dictionary of values\n",
    "    return {\n",
    "        \"X_train\": X_train,\n",
    "        \"X_valid\": X_valid,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_valid\": y_valid,\n",
    "        \"y_test\": y_test\n",
    "    }\n",
    "\n",
    "\n",
    "def create_model(vector_length=128):\n",
    "    \"\"\"5 hidden dense layers from 256 units to 64, not the best model, but not bad.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_shape=(vector_length,)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    # one output neuron with sigmoid activation function, 0 means female, 1 means male\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    # using binary crossentropy as it's male/female classification (binary)\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"], optimizer=\"adam\")\n",
    "    # print summary of the model\n",
    "    model.summary()\n",
    "    return model\n",
    "# load the dataset\n",
    "X, y = load_data()\n",
    "# split the data into training, validation and testing sets\n",
    "data = split_data(X, y, test_size=0.1, valid_size=0.1)\n",
    "# use tensorboard to view metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = split_data(X, y, test_size=0.1, valid_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_24 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 156,545\n",
      "Trainable params: 156,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "848/848 [==============================] - 13s 12ms/step - loss: 0.5597 - accuracy: 0.7686 - val_loss: 0.3942 - val_accuracy: 0.8473\n",
      "Epoch 2/100\n",
      "114/848 [===>..........................] - ETA: 8s - loss: 0.4340 - accuracy: 0.8261"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "# define early stopping to stop training after 5 epochs of not improving\n",
    "early_stopping =tf.keras.callbacks.EarlyStopping(mode=\"min\", patience=5, restore_best_weights=True)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "# train the model using the training set and validating using validation set\n",
    "model.fit(data[\"X_train\"], data[\"y_train\"], epochs=epochs, batch_size=batch_size, validation_data=(data[\"X_valid\"], data[\"y_valid\"]),\n",
    "          callbacks=[tensorboard, early_stopping])\n",
    "model.save(\"results/model.h5\")\n",
    "\n",
    "# evaluating the model using the testing set\n",
    "print(f\"Evaluating the model using {len(data['X_test'])} samples...\")\n",
    "loss, accuracy = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_30 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 156,545\n",
      "Trainable params: 156,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-9a19842bdf15>:38: FutureWarning: Pass y=[-0.04238397 -0.05017092 -0.03089473 ... -0.00126871 -0.01572455\n",
      " -0.02024781] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: female\n",
      "Probabilities::: Male: 9.37%    Female: 90.63%\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description=\"\"\"Gender recognition script, this will load the model you trained, \n",
    "                                    and perform inference on a sample you provide (either using your voice or a file)\"\"\")\n",
    "parser.add_argument(\"-f\", \"--file\", help=\"The path to the file, preferred to be in WAV format\")\n",
    "args = parser.parse_args()\n",
    "file = args.file\n",
    "# construct the model\n",
    "model = create_model()\n",
    "# load the saved/trained weights\n",
    "model.load_weights(\"results/model.h5\")\n",
    "if not file or not os.path.isfile(file):\n",
    "    # if file not provided, or it doesn't exist, use your voice\n",
    "    print(\"Please talk\")\n",
    "    # put the file name here\n",
    "    file = \"test.wav\"\n",
    "    # record the file (start talking)\n",
    "    record_to_file(file)\n",
    "# extract features and reshape it\n",
    "\n",
    "\n",
    "features = extract_feature(\"speech11.wav\", mel=True).reshape(1, -1)\n",
    "# predict the gender!\n",
    "male_prob = model.predict(features)[0][0]\n",
    "female_prob = 1 - male_prob\n",
    "gender = \"male\" if male_prob > female_prob else \"female\"\n",
    "# show the result!\n",
    "print(\"Result:\", gender)\n",
    "print(f\"Probabilities::: Male: {male_prob*100:.2f}%    Female: {female_prob*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as s_r\n",
    "def voice_input():\n",
    "    r = s_r.Recognizer()\n",
    "    my_mic = s_r.Microphone(device_index=1) #my device index is 1, you have to put your device index\n",
    "    with my_mic as source:\n",
    "        print(\"Say now!!!!\")\n",
    "        r.adjust_for_ambient_noise(source) #reduce noise\n",
    "        audio = r.listen(source) #take voice input from the microphone\n",
    "    with open(\"speech11.wav\",\"wb\") as f:\n",
    "        f.write(audio.get_wav_data())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/x4nth055/gender-recognition-by-voice/master/balanced-all.csv\" # Make sure the url is the raw version of the file on GitHub\n",
    "download = requests.get(url).content\n",
    "# Reading the downloaded content and turning it into a pandas dataframe\n",
    "df = pd.read_csv(io.StringIO(download.decode('utf-8')))\n",
    "# Printing out the first 5 rows of the dataframe\n",
    "print (df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
